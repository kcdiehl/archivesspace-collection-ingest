{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488cf94f",
   "metadata": {},
   "source": [
    "First, the script imports the Python packages/libraries needed to run script: pandas, json, argparse, uuid, & datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e8af446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "#import prefixLists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d0b043",
   "metadata": {},
   "source": [
    "Then, the script uses argparse to let us enter a filename in our terminal when we run the script. \n",
    "\n",
    "```\n",
    "python makeArchivalObjects.py -f filename.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0544c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-f', '--file')\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.file:\n",
    "    filename = args.file\n",
    "else:\n",
    "    filename = input('Enter filename (including \\'.csv\\'): ')\n",
    "filename = 'exampleSheets_archivalObjects.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16904b4e",
   "metadata": {},
   "source": [
    "Then, we see two functions:`add_to_dict` and `add_with_ref`. We can worry about these later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b647fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dict(dict_name, key, value):\n",
    "    try:\n",
    "        value = row.get(value)\n",
    "        if pd.notna(value):\n",
    "            value = value.strip()\n",
    "            dict_name[key] = value\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def add_with_ref(dict_name, key, value, repeat):  \n",
    "    try:\n",
    "        value = row[value]\n",
    "        if pd.notna(value):\n",
    "            if repeat == 'single':\n",
    "                value = value.strip()\n",
    "                dict_name[key] = {'ref': value}\n",
    "            else:\n",
    "                new_list = []\n",
    "                value = value.split('|')\n",
    "                for item in value:\n",
    "                    new_dict = {'ref': item}\n",
    "                    new_list.append(new_dict)\n",
    "                dict_name[key] = new_list\n",
    "    \n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    \n",
    "def add_notes(dict_name, value):\n",
    "    try:\n",
    "        value = row[value]\n",
    "        if pd.notna(value):\n",
    "            notes = []\n",
    "            notes_values = value.split('||')\n",
    "            for note_value in notes_values:\n",
    "                note_parts = note_value.split(';;') \n",
    "                first_model_type = note_parts[0]\n",
    "                note_type = note_parts[1]\n",
    "                second_model_type = note_parts[2]\n",
    "                content = note_parts[3]\n",
    "                note = {}\n",
    "                note['jsonmodel_type'] = first_model_type\n",
    "                note['publish'] = True\n",
    "                note['subnotes'] = [{'content': content, 'jsonmodel_type': second_model_type, 'publish': True}]\n",
    "                note['type'] = note_type\n",
    "                notes.append(note)\n",
    "            dict_name['notes'] = notes\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def add_extents(dict_name, value):\n",
    "    try:\n",
    "        value = row[value]\n",
    "        if pd.notna(value):\n",
    "            extent = []\n",
    "            extent_values = value.split('|')\n",
    "            extent_dict = {}\n",
    "            for extent_value in extent_values:\n",
    "                extent_value = extent_value.split(';;')\n",
    "                k = extent_value[0]\n",
    "                v = extent_value[1]\n",
    "                extent_dict[k] = v\n",
    "            extent.append(extent_dict)\n",
    "            dict_name['extent'] = extent\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82415b4",
   "metadata": {},
   "source": [
    "This next bit of code opens the CSV as a `DataFrame` called `df` and loops through its rows.\n",
    "\n",
    "As the script loops through each row, it extracts data based on CSV column names and adds the data to a dictionary called `json_file`. This dictionary will be transformed and saved as a JSON file at the end of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99778ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jsonmodel_type': 'archival_object', 'suppressed': False, 'title': 'Series 2: Digital photographs', 'resource': {'ref': '/repositories/3/resources/7'}, 'level': 'series', 'publish': True, 'restrictions_apply': True, 'repository_processing_note': \"Series 2 is currently unpublished in the finding aid. While what is included in series 2 is all processed, there is an agreed to 8 year restriction period from the time of creation and access jpegs have not yet been created. It would be a challenge to provide access to this scale of content in DSpace. What is currently in series 2 represents the processing of approximately 6,000 DVDs. There are approximately 10,000 more DVDs in accessions 2018-19.ua.007 and 2019-20.ua.012 that haven't been addressed. -EE 3/2/20\", 'position': '1', 'repository': {'ref': '/repositories/3'}, 'linked_events': [{'ref': '/repositories/3/events/10094'}, {'ref': ''}, {'ref': '/repositories/3/events/10097'}], 'subjects': [{'ref': '/subjects/1171'}, {'ref': '/subjects/490'}], 'notes': [{'jsonmodel_type': 'note_multipart', 'publish': True, 'subnotes': [{'content': \"Here's a note about the item.\", 'jsonmodel_type': 'note_text', 'publish': True}], 'type': 'phystech'}, {'jsonmodel_type': 'note_multipart', 'publish': True, 'subnotes': [{'content': \"Here's a note about the item.\", 'jsonmodel_type': 'note_text', 'publish': True}], 'type': 'arrangement'}], 'extent': [{'portion': 'whole', 'number': '13', 'extent_type': 'volumes', 'container_summary': 'Container info.'}]}\n",
      "{'jsonmodel_type': 'archival_object', 'suppressed': False, 'title': 'Achievement Rewards for College Scientists: Group photo', 'resource': {'ref': '/repositories/3/resources/7'}, 'level': 'file', 'publish': False, 'restrictions_apply': False, 'parent': {'ref': 'localAO_001'}, 'repository': {'ref': '/repositories/3'}}\n",
      "{'jsonmodel_type': 'archival_object', 'suppressed': False, 'title': 'Ann Peck Pugh letters', 'resource': {'ref': '/repositories/3/resources/661'}, 'level': 'series', 'publish': True, 'restrictions_apply': False, 'position': '43', 'repository': {'ref': '/repositories/3'}}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(filename, dtype={'position': int, 'parent': int})\n",
    "\n",
    "df['position'] = df['position'].astype(int)\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    # Create empty dictionary to store data.\n",
    "    json_file = {}\n",
    "    json_file['jsonmodel_type'] = 'archival_object'\n",
    "    json_file['suppressed'] = False\n",
    "    \n",
    "    # For required fields, add directly to json_file.\n",
    "    identifier = row['local_id']\n",
    "    json_file['title'] = row['title']\n",
    "    json_file['resource'] = {'ref': row['resource']}\n",
    "    json_file['level'] = row['level']\n",
    "    json_file['publish'] = row['publish']\n",
    "    json_file['restrictions_apply'] = row['restrictions_apply']\n",
    "    \n",
    "    # For optional fields, try to find value and add to json_file if found.\n",
    "    add_to_dict(json_file, 'repository_processing_note', 'repository_processing_note')\n",
    "    add_to_dict(json_file, 'position', 'position')\n",
    "    add_to_dict(json_file, 'other_level', 'other_level')\n",
    "    \n",
    "    # For optional fields with 'ref' key, use function to add.\n",
    "    add_with_ref(json_file, 'parent', 'parent', 'single')\n",
    "    add_with_ref(json_file, 'repository', 'repository', 'single')\n",
    "    add_with_ref(json_file, 'linked_events', 'linked_events', 'multi')\n",
    "    add_with_ref(json_file, 'subjects', 'subjects', 'multi')\n",
    "    \n",
    "    # Add notes.\n",
    "    add_notes(json_file, 'notes')\n",
    "    \n",
    "    # Add extent.\n",
    "    add_extents(json_file, 'extents')\n",
    "    print(json_file)\n",
    "    \n",
    "    dt = datetime.now().strftime('%Y-%m-%d')\n",
    "    ao_filename = identifier+'_'+dt+'.json'\n",
    "    directory = ''\n",
    "    with open(directory+ao_filename, 'w') as fp:\n",
    "        json.dump(json_file, fp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e838f9",
   "metadata": {},
   "source": [
    "This section generates a filename (`ao_filename`) based on an identifier variable and a datetime stamp, and then uses the json function `json.dump` to write and save our dictionary into a JSON file using our unique `ao_filename`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
